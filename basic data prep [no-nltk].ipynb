{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":127742,"databundleVersionId":15295088}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ntrain = pd.read_csv(\"/kaggle/input/comment-category-prediction-challenge/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/comment-category-prediction-challenge/test.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:08:44.598288Z","iopub.execute_input":"2026-02-14T18:08:44.598601Z","iopub.status.idle":"2026-02-14T18:08:47.529195Z","shell.execute_reply.started":"2026-02-14T18:08:44.598565Z","shell.execute_reply":"2026-02-14T18:08:47.528218Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/comment-category-prediction-challenge/Sample.csv\n/kaggle/input/comment-category-prediction-challenge/train.csv\n/kaggle/input/comment-category-prediction-challenge/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 0.  IMPORTS\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport gc\nimport copy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import issparse, csr_matrix\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.ensemble import VotingClassifier, BaggingClassifier, StackingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\ntry:\n    from xgboost import XGBClassifier\n    HAS_XGB = True\nexcept ImportError:\n    HAS_XGB = False; print(\"âš   xgboost not installed\")\n\ntry:\n    from lightgbm import LGBMClassifier\n    HAS_LGBM = True\nexcept ImportError:\n    HAS_LGBM = False; print(\"âš   lightgbm not installed\")\n\ntry:\n    import optuna\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    HAS_OPTUNA = True\nexcept ImportError:\n    HAS_OPTUNA = False; print(\"âš   optuna not installed â€” skipping tuning\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:08:47.530880Z","iopub.execute_input":"2026-02-14T18:08:47.531287Z","iopub.status.idle":"2026-02-14T18:08:50.708035Z","shell.execute_reply.started":"2026-02-14T18:08:47.531244Z","shell.execute_reply":"2026-02-14T18:08:50.707062Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1.  CONFIG  â† tune these to trade accuracy for memory\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTFIDF_MAX_FEATURES = 15_000   # 30k â†’ OOM; 10-15k is usually fine for F1\nTFIDF_NGRAM        = (1, 2)\nTUNING_TRIALS      = 15       # Optuna trials per model\nK_SHORTLIST        = 3        # top-K models to tune & ensemble\nRANDOM_STATE       = 42\nCV_FOLDS           = 2\n\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2.  LOAD DATA\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrain_raw = pd.read_csv(\"/kaggle/input/comment-category-prediction-challenge/train.csv\")\ntest_raw  = pd.read_csv(\"/kaggle/input/comment-category-prediction-challenge/test.csv\")\n\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 3.  FEATURE ENGINEERING\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nIDENTITY_CATS = [\"race\", \"religion\", \"gender\"]\n\ndef engineer_features(df: pd.DataFrame, ref_cols=None) -> pd.DataFrame:\n    df = df.copy()\n    df = df.drop(columns=[\"post_id\"], errors=\"ignore\")\n\n    df[\"created_date\"] = pd.to_datetime(df[\"created_date\"])\n    df[\"hour\"]         = df[\"created_date\"].dt.hour.astype(\"int8\")\n    df[\"dayofweek\"]    = df[\"created_date\"].dt.dayofweek.astype(\"int8\")\n    df[\"is_weekend\"]   = df[\"dayofweek\"].isin([5, 6]).astype(\"int8\")\n    df = df.drop(columns=[\"created_date\"])\n\n    df[IDENTITY_CATS] = df[IDENTITY_CATS].fillna(\"none\")\n    df = pd.get_dummies(df, columns=IDENTITY_CATS, drop_first=False, dtype=\"int8\")\n\n    # align to training schema\n    if ref_cols is not None:\n        for col in ref_cols:\n            if col not in df.columns:\n                df[col] = 0\n        df = df[ref_cols]\n\n    return df\n\n\ntrain_clean   = train_raw.dropna(subset=[\"comment\"]).reset_index(drop=True)\nfeatures      = engineer_features(train_clean.drop(columns=[\"label\"]))\nlabels        = train_clean[\"label\"].copy()\ntest_features = engineer_features(test_raw, ref_cols=features.columns.tolist())\n\n# free raw frames â€” we no longer need them\ndel train_raw\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:08:50.709438Z","iopub.execute_input":"2026-02-14T18:08:50.710656Z","iopub.status.idle":"2026-02-14T18:08:54.181284Z","shell.execute_reply.started":"2026-02-14T18:08:50.710610Z","shell.execute_reply":"2026-02-14T18:08:54.180398Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"25"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 4.  COLUMN GROUPS\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT_COL      = \"comment\"\nIDENTITY_COLS = [c for c in features.columns if c.startswith((\"race_\", \"religion_\", \"gender_\"))]\nREACTION_COLS = [\"upvote\", \"downvote\", \"emoticon_1\", \"emoticon_2\", \"emoticon_3\"]\nIF_COLS       = [\"if_1\", \"if_2\"]\nTIME_COLS     = [\"hour\", \"dayofweek\", \"is_weekend\"]\nNUM_CLASSES   = labels.nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:08:54.183116Z","iopub.execute_input":"2026-02-14T18:08:54.183691Z","iopub.status.idle":"2026-02-14T18:08:54.190141Z","shell.execute_reply.started":"2026-02-14T18:08:54.183654Z","shell.execute_reply":"2026-02-14T18:08:54.189229Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 5.  PREPROCESSOR FACTORY\n#     sparse_threshold=1.0  â†’ ALWAYS keeps sparse output (critical!)\n#     dtype=float32         â†’ halves RAM vs float64\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef make_tfidf():\n    return TfidfVectorizer(\n        lowercase=True, stop_words=\"english\",\n        max_features=TFIDF_MAX_FEATURES, ngram_range=TFIDF_NGRAM,\n        min_df=5, max_df=0.9, dtype=np.float32)\n\n\ndef make_preprocessor():\n    return ColumnTransformer(\n        transformers=[\n            (\"text\",     make_tfidf(),           TEXT_COL),\n            (\"identity\", \"passthrough\",           IDENTITY_COLS),\n            (\"reaction\", MinMaxScaler(),          REACTION_COLS),\n            (\"ifs\",      MinMaxScaler(),          IF_COLS),\n            (\"time\",     MinMaxScaler(),          TIME_COLS),\n        ],\n        sparse_threshold=1.0,   # â† NEVER densify\n        n_jobs=1,\n    )\n\n\ndef make_pipe():\n    return Pipeline([(\"pre\", make_preprocessor())])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:08:54.191401Z","iopub.execute_input":"2026-02-14T18:08:54.191731Z","iopub.status.idle":"2026-02-14T18:08:54.212088Z","shell.execute_reply.started":"2026-02-14T18:08:54.191693Z","shell.execute_reply":"2026-02-14T18:08:54.211034Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 6.  TRAIN / VAL SPLIT\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nidx = np.arange(len(features))\nidx_tr, idx_val, y_tr, y_val = train_test_split(\n    idx, labels.values,\n    test_size=0.2, stratify=labels.values, random_state=RANDOM_STATE\n)\nX_tr  = features.loc[idx_tr]\nX_val = features.loc[idx_val]\n\n# Fit preprocessor ONCE and keep sparse matrices\n_pipe = make_pipe()\nXtr_full  = _pipe.fit_transform(X_tr)   # scipy sparse, float32\nXval_full = _pipe.transform(X_val)      # scipy sparse, float32\nprint(f\"âœ“ Preprocessed  train: {Xtr_full.shape}  val: {Xval_full.shape}\")\nprint(f\"  Train sparse nnz: {Xtr_full.nnz:,}  (~{Xtr_full.nnz*4/1e6:.0f} MB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:08:54.213615Z","iopub.execute_input":"2026-02-14T18:08:54.214057Z","iopub.status.idle":"2026-02-14T18:09:23.523992Z","shell.execute_reply.started":"2026-02-14T18:08:54.214018Z","shell.execute_reply":"2026-02-14T18:09:23.523090Z"}},"outputs":[{"name":"stdout","text":"âœ“ Preprocessed  train: (158399, 15029)  val: (39600, 15029)\n  Train sparse nnz: 4,641,354  (~19 MB)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 7.  SPARSE â†’ DENSE HELPER  (only called for tree-based models)\n#     Converts in float32, then deletes the dense copy after use\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef to_dense_f32(X):\n    \"\"\"Convert sparse to float32 dense array. Caller must del result + gc.\"\"\"\n    arr = X.toarray().astype(np.float32) if issparse(X) else X.astype(np.float32)\n    return arr\n\nDENSE_MODELS = {\"XGBoost\", \"RandomForest\"}\n\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 8.  MODEL REGISTRY  â† ADD / REMOVE MODELS HERE\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMODEL_REGISTRY = {\n    \"LogisticRegression\": LogisticRegression(\n        C=1.0, max_iter=2000, solver=\"saga\", n_jobs=-1),\n\n    # CalibratedClassifierCV wraps LinearSVC â†’ gives predict_proba for ensembles\n    \"LinearSVC\": CalibratedClassifierCV(\n        LinearSVC(C=1.0, class_weight=\"balanced\", max_iter=5000)),\n\n    # ComplementNB: sparse-native, great for imbalanced text\n    \"ComplementNB\": ComplementNB(alpha=0.1),\n}\n\nif HAS_XGB:\n    MODEL_REGISTRY[\"XGBoost\"] = XGBClassifier(\n        objective=\"multi:softmax\", num_class=NUM_CLASSES,\n        n_estimators=300, max_depth=6, learning_rate=0.1,\n        subsample=0.8, colsample_bytree=0.8,\n        tree_method=\"hist\", device=\"cpu\",\n        eval_metric=\"mlogloss\", n_jobs=-1, random_state=RANDOM_STATE)\n\nif HAS_LGBM:\n    MODEL_REGISTRY[\"LightGBM\"] = LGBMClassifier(\n        n_estimators=300, num_leaves=63, learning_rate=0.05,\n        subsample=0.8, colsample_bytree=0.8,\n        n_jobs=-1, random_state=RANDOM_STATE, verbose=-1)\n\n# â”€â”€ To add a new model, just append here: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# from sklearn.linear_model import SGDClassifier\n# MODEL_REGISTRY[\"SGD\"] = SGDClassifier(loss=\"modified_huber\", n_jobs=-1)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:09:23.525208Z","iopub.execute_input":"2026-02-14T18:09:23.525576Z","iopub.status.idle":"2026-02-14T18:09:23.535349Z","shell.execute_reply.started":"2026-02-14T18:09:23.525540Z","shell.execute_reply":"2026-02-14T18:09:23.534204Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 9.  EVALUATION HELPERS\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef evaluate(clf, X_v, y_v, name=\"\", verbose=True):\n    preds    = clf.predict(X_v)\n    macro_f1 = f1_score(y_v, preds, average=\"macro\")\n    if verbose:\n        print(f\"\\n{name}\")\n        print(classification_report(y_v, preds, zero_division=0))\n    return macro_f1\n\ndef fit_eval(name, clf, Xtr, Xv, ytr, yv, verbose=True):\n    needs_dense = any(tag in name for tag in DENSE_MODELS)\n\n    if needs_dense:\n        Xtr_in = to_dense_f32(Xtr)\n        clf.fit(Xtr_in, ytr)\n        del Xtr_in; gc.collect()\n        Xv_in = to_dense_f32(Xv)\n        f1 = evaluate(clf, Xv_in, yv, name=name, verbose=verbose)\n        del Xv_in; gc.collect()\n\n    elif name == \"LightGBM\":\n        import lightgbm as lgb\n        clf.fit(Xtr, ytr,\n                eval_set=[(Xv, yv)],\n                callbacks=[lgb.early_stopping(40, verbose=False),\n                           lgb.log_evaluation(period=-1)])\n        f1 = evaluate(clf, Xv, yv, name=name, verbose=verbose)\n        print(f\"  â†’ Best n_estimators found: {clf.best_iteration_}\")\n\n    else:\n        clf.fit(Xtr, ytr)\n        f1 = evaluate(clf, Xv, yv, name=name, verbose=verbose)\n\n    return clf, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:09:23.536509Z","iopub.execute_input":"2026-02-14T18:09:23.536868Z","iopub.status.idle":"2026-02-14T18:09:23.560137Z","shell.execute_reply.started":"2026-02-14T18:09:23.536832Z","shell.execute_reply":"2026-02-14T18:09:23.559166Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 10. EVALUATE ALL MODELS\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nresults = {}\n\nprint(\"\\n\" + \"â•\"*60)\nprint(\"  FULL FEATURE SET â€” ALL MODELS\")\nprint(\"â•\"*60)\n\nfor name, clf in MODEL_REGISTRY.items():\n    print(f\"\\nâ–¶  {name}\")\n    trained, score = fit_eval(name, copy.deepcopy(clf),\n                              Xtr_full, Xval_full, y_tr, y_val)\n    results[name] = {\"score\": score, \"model\": trained}\n    gc.collect()\n\nranked    = sorted(results.items(), key=lambda x: x[1][\"score\"], reverse=True)\nshortlist = [n for n, _ in ranked[:K_SHORTLIST]]\n\nprint(\"\\n\" + \"â•\"*60)\nprint(\"  MACRO-F1 LEADERBOARD\")\nprint(\"â•\"*60)\nfor i, (n, info) in enumerate(ranked, 1):\n    tag = \" âœ“\" if n in shortlist else \"\"\n    print(f\"  {i}. {n:<25} {info['score']:.4f}{tag}\")\nprint(f\"\\n  Shortlisted: {shortlist}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:09:23.561187Z","iopub.execute_input":"2026-02-14T18:09:23.561478Z","iopub.status.idle":"2026-02-14T20:10:03.942202Z","shell.execute_reply.started":"2026-02-14T18:09:23.561452Z","shell.execute_reply":"2026-02-14T20:10:03.940166Z"}},"outputs":[{"name":"stdout","text":"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  FULL FEATURE SET â€” ALL MODELS\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ–¶  LogisticRegression\n\nLogisticRegression\n              precision    recall  f1-score   support\n\n           0       0.80      0.92      0.85     22834\n           1       0.72      0.61      0.66      3184\n           2       0.82      0.67      0.74     12488\n           3       0.68      0.31      0.43      1094\n\n    accuracy                           0.80     39600\n   macro avg       0.76      0.63      0.67     39600\nweighted avg       0.80      0.80      0.79     39600\n\n\nâ–¶  LinearSVC\n\nLinearSVC\n              precision    recall  f1-score   support\n\n           0       0.80      0.91      0.85     22834\n           1       0.73      0.58      0.65      3184\n           2       0.80      0.68      0.73     12488\n           3       0.70      0.28      0.40      1094\n\n    accuracy                           0.79     39600\n   macro avg       0.76      0.61      0.66     39600\nweighted avg       0.79      0.79      0.78     39600\n\n\nâ–¶  ComplementNB\n\nComplementNB\n              precision    recall  f1-score   support\n\n           0       0.82      0.73      0.77     22834\n           1       0.45      0.86      0.59      3184\n           2       0.66      0.65      0.66     12488\n           3       0.50      0.37      0.43      1094\n\n    accuracy                           0.71     39600\n   macro avg       0.61      0.65      0.61     39600\nweighted avg       0.73      0.71      0.71     39600\n\n\nâ–¶  XGBoost\n\nXGBoost\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.96     22834\n           1       0.77      0.76      0.76      3184\n           2       0.83      0.92      0.87     12488\n           3       0.76      0.39      0.51      1094\n\n    accuracy                           0.91     39600\n   macro avg       0.83      0.75      0.78     39600\nweighted avg       0.91      0.91      0.91     39600\n\n\nâ–¶  LightGBM\n\nLightGBM\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.96     22834\n           1       0.78      0.79      0.78      3184\n           2       0.85      0.92      0.88     12488\n           3       0.75      0.52      0.61      1094\n\n    accuracy                           0.91     39600\n   macro avg       0.84      0.79      0.81     39600\nweighted avg       0.92      0.91      0.91     39600\n\n  â†’ Best n_estimators found: 288\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  MACRO-F1 LEADERBOARD\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  1. LightGBM                  0.8102 âœ“\n  2. XGBoost                   0.7774 âœ“\n  3. LogisticRegression        0.6709 âœ“\n  4. LinearSVC                 0.6596\n  5. ComplementNB              0.6112\n\n  Shortlisted: ['LightGBM', 'XGBoost', 'LogisticRegression']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\nimport pickle, os\n\nSAVE_DIR = \"/kaggle/working/saved_models\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n\n# Add this at the end of section 10, after computing Xtr_full / Xval_full\nfrom scipy.sparse import save_npz, load_npz\n\nsave_npz(f\"{SAVE_DIR}/Xtr_full.npz\", Xtr_full)\nsave_npz(f\"{SAVE_DIR}/Xval_full.npz\", Xval_full)\nnp.save(f\"{SAVE_DIR}/y_tr.npy\", y_tr)\nnp.save(f\"{SAVE_DIR}/y_val.npy\", y_val)\nprint(\"ğŸ’¾ Preprocessed matrices saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T20:13:59.173133Z","iopub.execute_input":"2026-02-14T20:13:59.173516Z","iopub.status.idle":"2026-02-14T20:14:06.596607Z","shell.execute_reply.started":"2026-02-14T20:13:59.173487Z","shell.execute_reply":"2026-02-14T20:14:06.595727Z"}},"outputs":[{"name":"stdout","text":"ğŸ’¾ Preprocessed matrices saved\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from scipy.sparse import load_npz\n\n_xtr_path = f\"{SAVE_DIR}/Xtr_full.npz\"\nif os.path.exists(_xtr_path):\n    Xtr_full  = load_npz(f\"{SAVE_DIR}/Xtr_full.npz\")\n    Xval_full = load_npz(f\"{SAVE_DIR}/Xval_full.npz\")\n    y_tr      = np.load(f\"{SAVE_DIR}/y_tr.npy\")\n    y_val     = np.load(f\"{SAVE_DIR}/y_val.npy\")\n    print(\"âœ… Loaded preprocessed matrices from cache\")\nelse:\n    print(\"âš  No cached matrices found â€” run sections 1â€“10 first\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T20:14:34.475629Z","iopub.execute_input":"2026-02-14T20:14:34.476035Z","iopub.status.idle":"2026-02-14T20:14:34.948783Z","shell.execute_reply.started":"2026-02-14T20:14:34.476003Z","shell.execute_reply":"2026-02-14T20:14:34.947828Z"}},"outputs":[{"name":"stdout","text":"âœ… Loaded preprocessed matrices from cache\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 11. HYPERPARAMETER TUNING  (RandomizedSearchCV â€” sklearn only)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import loguniform, randint, uniform\nimport pickle, os, lightgbm as lgb\nfrom scipy.sparse import save_npz, load_npz\n\nSAVE_DIR = \"/kaggle/working/saved_models\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# â”€â”€ Save/load helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef save_model(name, clf, score):\n    with open(f\"{SAVE_DIR}/{name}.pkl\", \"wb\") as f:\n        pickle.dump({\"model\": clf, \"score\": score}, f)\n    print(f\"  ğŸ’¾ Saved {name}\")\n\ndef load_model(name):\n    path = f\"{SAVE_DIR}/{name}.pkl\"\n    if os.path.exists(path):\n        with open(path, \"rb\") as f:\n            return pickle.load(f)\n    return None\n\n# â”€â”€ Save preprocessed matrices (so future restarts skip section 6) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsave_npz(f\"{SAVE_DIR}/Xtr_full.npz\", Xtr_full)\nsave_npz(f\"{SAVE_DIR}/Xval_full.npz\", Xval_full)\nnp.save(f\"{SAVE_DIR}/y_tr.npy\", y_tr)\nnp.save(f\"{SAVE_DIR}/y_val.npy\", y_val)\nprint(\"ğŸ’¾ Preprocessed matrices saved\")\n\n# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTUNING_ITER  = 15\nCV_FOLDS     = 2\nSKIP_TUNING  = {\"XGBoost\"}   # default params are good enough; saves hours\n\n# â”€â”€ LightGBM: special tuning with early stopping (faster than CV) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef tune_lgbm(Xtr, Xv, ytr, yv, n_iter=TUNING_ITER):\n    \"\"\"\n    Manual random search for LightGBM using a single val split + early stopping.\n    Much faster than CV because bad configs die early.\n    \"\"\"\n    from scipy.stats import randint, uniform\n    import numpy as np\n\n    rng = np.random.RandomState(RANDOM_STATE)\n    best_score, best_clf = -1, None\n\n    for i in range(n_iter):\n        params = dict(\n            num_leaves      = int(rng.randint(31, 127)),\n            learning_rate   = float(np.exp(rng.uniform(np.log(0.03), np.log(0.2)))),\n            subsample       = float(rng.uniform(0.6, 1.0)),\n            colsample_bytree= float(rng.uniform(0.6, 1.0)),\n        )\n        clf = LGBMClassifier(\n            n_estimators=500, **params,\n            n_jobs=-1, random_state=RANDOM_STATE, verbose=-1)\n        clf.fit(Xtr, ytr,\n                eval_set=[(Xv, yv)],\n                callbacks=[lgb.early_stopping(40, verbose=False),\n                           lgb.log_evaluation(period=-1)])\n        score = f1_score(yv, clf.predict(Xv), average=\"macro\")\n        print(f\"    iter {i+1:02d}  leaves={params['num_leaves']:3d}  \"\n              f\"lr={params['learning_rate']:.3f}  f1={score:.4f}\"\n              f\"  trees={clf.best_iteration_}\")\n        if score > best_score:\n            best_score, best_clf = score, clf\n            print(f\"    âœ“ New best: {best_score:.4f}\")\n\n    return best_clf, best_score\n\n# â”€â”€ Search spaces for RandomizedSearchCV (non-LightGBM models) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nPARAM_GRIDS = {\n    \"LogisticRegression\": {\n        \"C\": loguniform(1e-2, 10),\n    },\n    \"LinearSVC\": {\n        \"base_estimator__C\":            loguniform(1e-2, 10),\n        \"base_estimator__class_weight\": [\"balanced\", None],\n    },\n    \"ComplementNB\": {\n        \"alpha\": loguniform(1e-3, 2.0),\n    },\n    \"XGBoost\": {\n        \"n_estimators\":     [100, 200, 300],\n        \"max_depth\":        randint(3, 7),\n        \"learning_rate\":    [0.05, 0.1, 0.2],\n        \"subsample\":        uniform(0.6, 0.4),\n        \"colsample_bytree\": uniform(0.6, 0.4),\n    },\n}\n\n# â”€â”€ Main tuning loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntuned_models = {}\n\nfor name in shortlist:\n    print(f\"\\n{'â”€'*60}\\n  Tuning: {name}\\n{'â”€'*60}\")\n\n    # check cache first\n    cached = load_model(name)\n    if cached is not None:\n        print(f\"  âœ… Loaded from cache â€” macro-F1: {cached['score']:.4f}\")\n        tuned_models[name] = cached\n        continue\n\n    # LightGBM: custom early-stopping random search\n    if name == \"LightGBM\":\n        trained, score = tune_lgbm(Xtr_full, Xval_full, y_tr, y_val)\n        evaluate(trained, Xval_full, y_val, name=name, verbose=True)\n\n    # XGBoost / anything in SKIP_TUNING: use defaults\n    elif name in SKIP_TUNING:\n        print(\"  Skipping tuning â€” using defaults\")\n        trained, score = fit_eval(name, copy.deepcopy(MODEL_REGISTRY[name]),\n                                  Xtr_full, Xval_full, y_tr, y_val)\n\n    # Everything else: RandomizedSearchCV\n    else:\n        needs_dense = any(tag in name for tag in DENSE_MODELS)\n        Xtr_in = to_dense_f32(Xtr_full) if needs_dense else Xtr_full\n\n        search = RandomizedSearchCV(\n            estimator=copy.deepcopy(MODEL_REGISTRY[name]),\n            param_distributions=PARAM_GRIDS.get(name, {}),\n            n_iter=TUNING_ITER, scoring=\"f1_macro\",\n            cv=CV_FOLDS, n_jobs=1, refit=True,\n            random_state=RANDOM_STATE, verbose=1, error_score=\"raise\")\n        search.fit(Xtr_in, y_tr)\n\n        if needs_dense:\n            del Xtr_in; gc.collect()\n\n        print(f\"  Best CV macro-F1: {search.best_score_:.4f}\")\n        print(f\"  Best params:      {search.best_params_}\")\n\n        trained = search.best_estimator_\n        Xv_in   = to_dense_f32(Xval_full) if needs_dense else Xval_full\n        score   = evaluate(trained, Xv_in, y_val, name=name, verbose=True)\n        if needs_dense:\n            del Xv_in; gc.collect()\n\n    tuned_models[name] = {\"model\": trained, \"score\": score}\n    save_model(name, trained, score)\n    print(f\"  âœ“ Val macro-F1: {score:.4f}\")\n    gc.collect()\n\nprint(\"\\n\" + \"â•\"*60)\nprint(\"  TUNED SCORES\")\nprint(\"â•\"*60)\nfor n, info in sorted(tuned_models.items(), key=lambda x: x[1][\"score\"], reverse=True):\n    print(f\"  {n:<25} {info['score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T20:20:23.832405Z","iopub.execute_input":"2026-02-14T20:20:23.832829Z","iopub.status.idle":"2026-02-15T00:56:31.809836Z","shell.execute_reply.started":"2026-02-14T20:20:23.832798Z","shell.execute_reply":"2026-02-15T00:56:31.807921Z"}},"outputs":[{"name":"stdout","text":"ğŸ’¾ Preprocessed matrices saved\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Tuning: LightGBM\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    iter 01  leaves= 82  lr=0.182  f1=0.8057  trees=65\n    âœ“ New best: 0.8057\n    iter 02  leaves=113  lr=0.036  f1=0.8113  trees=285\n    âœ“ New best: 0.8113\n    iter 03  leaves= 54  lr=0.103  f1=0.8072  trees=163\n    iter 04  leaves= 60  lr=0.045  f1=0.8096  trees=351\n    iter 05  leaves=106  lr=0.096  f1=0.8090  trees=112\n    iter 06  leaves=121  lr=0.096  f1=0.8096  trees=98\n    iter 07  leaves=110  lr=0.047  f1=0.8109  trees=222\n    iter 08  leaves= 85  lr=0.194  f1=0.8058  trees=59\n    iter 09  leaves= 37  lr=0.041  f1=0.8060  trees=500\n    iter 10  leaves= 90  lr=0.087  f1=0.8103  trees=127\n    iter 11  leaves= 32  lr=0.110  f1=0.8064  trees=257\n    iter 12  leaves= 38  lr=0.032  f1=0.8048  trees=500\n    iter 13  leaves= 34  lr=0.054  f1=0.8086  trees=493\n    iter 14  leaves= 84  lr=0.032  f1=0.8120  trees=392\n    âœ“ New best: 0.8120\n    iter 15  leaves=104  lr=0.164  f1=0.8048  trees=58\n\nLightGBM\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.96     22834\n           1       0.77      0.79      0.78      3184\n           2       0.85      0.92      0.88     12488\n           3       0.74      0.53      0.62      1094\n\n    accuracy                           0.91     39600\n   macro avg       0.84      0.80      0.81     39600\nweighted avg       0.92      0.91      0.91     39600\n\n  ğŸ’¾ Saved LightGBM\n  âœ“ Val macro-F1: 0.8120\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Tuning: XGBoost\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Skipping tuning â€” using defaults\n\nXGBoost\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.96     22834\n           1       0.77      0.76      0.76      3184\n           2       0.83      0.92      0.87     12488\n           3       0.76      0.39      0.51      1094\n\n    accuracy                           0.91     39600\n   macro avg       0.83      0.75      0.78     39600\nweighted avg       0.91      0.91      0.91     39600\n\n  ğŸ’¾ Saved XGBoost\n  âœ“ Val macro-F1: 0.7774\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Tuning: LogisticRegression\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nFitting 2 folds for each of 15 candidates, totalling 30 fits\n  Best CV macro-F1: 0.6636\n  Best params:      {'C': np.float64(8.123245085588687)}\n\nLogisticRegression\n              precision    recall  f1-score   support\n\n           0       0.83      0.91      0.87     22834\n           1       0.71      0.63      0.67      3184\n           2       0.82      0.73      0.77     12488\n           3       0.62      0.40      0.49      1094\n\n    accuracy                           0.82     39600\n   macro avg       0.74      0.67      0.70     39600\nweighted avg       0.81      0.82      0.81     39600\n\n  ğŸ’¾ Saved LogisticRegression\n  âœ“ Val macro-F1: 0.6975\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  TUNED SCORES\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  LightGBM                  0.8120\n  XGBoost                   0.7774\n  LogisticRegression        0.6975\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 12. ENSEMBLE\n#     Only sparse-native models enter VotingClassifier / Stacking\n#     to avoid double-materialising dense matrices.\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsparse_native = [(n, info[\"model\"]) for n, info in tuned_models.items()\n                 if not any(tag in n for tag in DENSE_MODELS)]\nall_tuned     = [(n, info[\"model\"]) for n, info in tuned_models.items()]\n\nensemble_scores = {}\n\n# â”€â”€ 12a. Soft voting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif len(sparse_native) >= 2:\n    print(\"\\n\" + \"â•\"*60 + \"\\n  ENSEMBLE: SOFT VOTING\\n\" + \"â•\"*60)\n    voter = VotingClassifier(estimators=sparse_native, voting=\"soft\", n_jobs=1)\n    voter.fit(Xtr_full, y_tr)\n    voter_f1 = evaluate(voter, Xval_full, y_val, name=\"SoftVoting\")\n    ensemble_scores[\"SoftVoting\"] = voter_f1\n    print(f\"  Soft-vote macro-F1: {voter_f1:.4f}\")\n    gc.collect()\nelse:\n    voter = None\n    print(\"\\nâš   Not enough sparse-native models for soft voting.\")\n\n# â”€â”€ 12b. Stacking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif len(sparse_native) >= 2:\n    print(\"\\n\" + \"â•\"*60 + \"\\n  ENSEMBLE: STACKING\\n\" + \"â•\"*60)\n    stacker = StackingClassifier(\n        estimators=sparse_native,\n        final_estimator=LogisticRegression(C=1.0, max_iter=2000, solver=\"saga\"),\n        cv=3, n_jobs=1, passthrough=False)\n    stacker.fit(Xtr_full, y_tr)\n    stack_f1 = evaluate(stacker, Xval_full, y_val, name=\"Stacking\")\n    ensemble_scores[\"Stacking\"] = stack_f1\n    print(f\"  Stacking macro-F1: {stack_f1:.4f}\")\n    gc.collect()\nelse:\n    stacker = None\n\n# â”€â”€ 12c. Bagging around best sparse-native model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif sparse_native:\n    best_sparse_name = max(\n        (n for n in tuned_models if not any(t in n for t in DENSE_MODELS)),\n        key=lambda n: tuned_models[n][\"score\"])\n    print(f\"\\n\" + \"â•\"*60 + f\"\\n  ENSEMBLE: BAGGING ({best_sparse_name})\\n\" + \"â•\"*60)\n    bagger = BaggingClassifier(\n        estimator=copy.deepcopy(MODEL_REGISTRY[best_sparse_name]),\n        n_estimators=10, max_samples=0.8, max_features=1.0,\n        n_jobs=1, random_state=RANDOM_STATE)\n    bagger.fit(Xtr_full, y_tr)\n    bag_f1 = evaluate(bagger, Xval_full, y_val, name=\"Bagging\")\n    ensemble_scores[\"Bagging\"] = bag_f1\n    print(f\"  Bagging macro-F1: {bag_f1:.4f}\")\n    gc.collect()\nelse:\n    bagger = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T00:56:31.813480Z","iopub.execute_input":"2026-02-15T00:56:31.813795Z","iopub.status.idle":"2026-02-15T04:26:43.574730Z","shell.execute_reply.started":"2026-02-15T00:56:31.813770Z","shell.execute_reply":"2026-02-15T04:26:43.573364Z"}},"outputs":[{"name":"stdout","text":"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  ENSEMBLE: SOFT VOTING\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSoftVoting\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94     22834\n           1       0.78      0.74      0.76      3184\n           2       0.87      0.87      0.87     12488\n           3       0.74      0.46      0.57      1094\n\n    accuracy                           0.90     39600\n   macro avg       0.83      0.76      0.79     39600\nweighted avg       0.89      0.90      0.89     39600\n\n  Soft-vote macro-F1: 0.7852\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  ENSEMBLE: STACKING\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nStacking\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96     22834\n           1       0.80      0.76      0.78      3184\n           2       0.85      0.92      0.89     12488\n           3       0.75      0.53      0.62      1094\n\n    accuracy                           0.91     39600\n   macro avg       0.84      0.79      0.81     39600\nweighted avg       0.91      0.91      0.91     39600\n\n  Stacking macro-F1: 0.8107\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  ENSEMBLE: BAGGING (LightGBM)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nBagging\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.96     22834\n           1       0.78      0.78      0.78      3184\n           2       0.85      0.92      0.88     12488\n           3       0.76      0.50      0.60      1094\n\n    accuracy                           0.91     39600\n   macro avg       0.84      0.79      0.81     39600\nweighted avg       0.92      0.91      0.91     39600\n\n  Bagging macro-F1: 0.8077\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 13. PICK WINNER + RETRAIN ON FULL DATA\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nall_scores = {n: info[\"score\"] for n, info in tuned_models.items()}\nall_scores.update(ensemble_scores)\n\nbest_name = max(all_scores, key=all_scores.get)\nprint(f\"\\nğŸ†  Winner: {best_name}  (macro-F1 = {all_scores[best_name]:.4f})\")\n\n# map name â†’ fitted object\nfitted_map = {n: info[\"model\"] for n, info in tuned_models.items()}\nif voter   is not None: fitted_map[\"SoftVoting\"] = voter\nif stacker is not None: fitted_map[\"Stacking\"]   = stacker\nif bagger  is not None: fitted_map[\"Bagging\"]    = bagger\n\n# â”€â”€ Retrain on ALL training data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\nğŸ”„  Retraining on full training data â€¦\")\npipe_final  = make_pipe()\nX_all_pre   = pipe_final.fit_transform(features)   # sparse float32\nX_test_pre  = pipe_final.transform(test_features)  # sparse float32\n\n# Determine what the winning model actually is for the retrain\nif best_name in tuned_models:\n    # single model â€” get best params from study if available\n    final_clf = copy.deepcopy(fitted_map[best_name])\nelse:\n    # ensemble â€” retrain its component models from scratch on full data\n    # (the ensemble itself was trained on idx_tr only)\n    final_clf = copy.deepcopy(fitted_map[best_name])\n\nneeds_dense = any(tag in best_name for tag in DENSE_MODELS)\n\nif needs_dense:\n    X_all_dense  = to_dense_f32(X_all_pre);  del X_all_pre;  gc.collect()\n    X_test_dense = to_dense_f32(X_test_pre); del X_test_pre; gc.collect()\n    final_clf.fit(X_all_dense, labels)\n    test_preds = final_clf.predict(X_test_dense)\n    del X_all_dense, X_test_dense; gc.collect()\nelse:\n    final_clf.fit(X_all_pre, labels)\n    test_preds = final_clf.predict(X_test_pre)\n    del X_all_pre, X_test_pre; gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T04:26:43.576293Z","iopub.execute_input":"2026-02-15T04:26:43.577019Z","iopub.status.idle":"2026-02-15T04:54:01.987806Z","shell.execute_reply.started":"2026-02-15T04:26:43.576977Z","shell.execute_reply":"2026-02-15T04:54:01.986877Z"}},"outputs":[{"name":"stdout","text":"\nğŸ†  Winner: LightGBM  (macro-F1 = 0.8120)\n\nğŸ”„  Retraining on full training data â€¦\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 14. SUBMISSION\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsubmission = pd.DataFrame({\n    \"ID\":    range(1, len(test_raw) + 1),\n    \"label\": test_preds\n})\nassert len(submission) == len(test_raw), \"âŒ Row count mismatch!\"\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(f\"\\nâœ…  submission.csv saved  ({len(submission)} rows)\")\nprint(f\"   Label distribution:\\n{submission['label'].value_counts().to_string()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T04:54:01.990143Z","iopub.execute_input":"2026-02-15T04:54:01.990473Z","iopub.status.idle":"2026-02-15T04:54:02.095150Z","shell.execute_reply.started":"2026-02-15T04:54:01.990445Z","shell.execute_reply":"2026-02-15T04:54:02.093857Z"}},"outputs":[{"name":"stdout","text":"\nâœ…  submission.csv saved  (102000 rows)\n   Label distribution:\nlabel\n0    56942\n2    34846\n1     8219\n3     1993\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}